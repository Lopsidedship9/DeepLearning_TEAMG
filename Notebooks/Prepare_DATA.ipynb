{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation for Model Training\n",
        "\n",
        "This notebook provides tools to prepare audio data for training a Convolutional Neural Network (CNN). You may use a subset of the dataset available on Kaggle: https://www.kaggle.com/datasets/defined/birdclef-2021-mel-spectograms, or alternatively, the original **BirdClef** dataset. If you choose the latter, it is necessary to convert each audio file into a **Mel-Spectrogram** before proceeding.\n",
        "\n",
        "A preprocessed dataset is already provided for training purposes. However, if you decide to use new or alternative data, it is important to consider the timing of bird vocalizations within the recordings. Ideally, a statistical analysis should be performed to determine whether bird calls tend to occur near the midpoint of the audio files. If this assumption does not hold, the current data processing pipeline may not behave as intended.\n",
        "\n"
      ],
      "metadata": {
        "id": "OhncEfLjvQLO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5ryc7SSvBE1",
        "outputId": "c4ac61fa-d881-4185-e180-a5334a550f09"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "dir = '/content/drive/Shared drives/Deep Learning Group G/UPF_Deep_Learning_2025/Final Project/Birdclef2021/' # Your path here.\n",
        "os.chdir(dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#See if you have each bird folder in the raight place.\n",
        "folder_path = dir + \"train/\" #Change to the data path\n",
        "\n",
        "def list_directory_contents(path):\n",
        "    if not os.path.exists(path):\n",
        "        print(\"The path does not exist.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Contents of: {path}\\n\")\n",
        "    for item in os.listdir(path):\n",
        "        item_path = os.path.join(path, item)\n",
        "        if os.path.isdir(item_path):\n",
        "            print(f\"[DIR]  {item}\")\n",
        "        else:\n",
        "            print(f\"[FILE] {item}\")\n",
        "\n",
        "list_directory_contents(folder_path)"
      ],
      "metadata": {
        "id": "4IMWRiUQzekT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87152ae9-8268-4191-ee9d-2cc7a5611af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Contents of: /content/drive/Shared drives/Deep Learning Group G/UPF_Deep_Learning_2025/Final Project/Birdclef2021/train/\n",
            "\n",
            "[DIR]  redcro\n",
            "[DIR]  gbwwre1\n",
            "[DIR]  norcar\n",
            "[DIR]  comrav\n",
            "[DIR]  sonspa\n",
            "[DIR]  houspa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_fragments_per_file(folder_path):\n",
        "    \"\"\"Returns list of tuples (filename, number of fragments) for all .npy files in a folder.\"\"\"\n",
        "    files_fragments = []\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.npy'):\n",
        "            full_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                data = np.load(full_path)\n",
        "                if len(data.shape) == 3:\n",
        "                    files_fragments.append((filename, data.shape[0]))\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                continue\n",
        "    return files_fragments\n",
        "\n",
        "def calculate_folder_mean(files_fragments):\n",
        "    \"\"\"Returns floor mean of fragment counts.\"\"\"\n",
        "    fragment_counts = [n for _, n in files_fragments]\n",
        "    if not fragment_counts:\n",
        "        return None\n",
        "    return int(np.floor(np.mean(fragment_counts)))\n",
        "\n",
        "def move_and_trim_files_to_target(src_folder, dst_root, files_fragments, global_mean, target_length):\n",
        "    \"\"\"Copies and trims files directly to dst_root, flattening the structure.\"\"\"\n",
        "    min_acceptable = global_mean - 2\n",
        "    max_acceptable = global_mean + 4\n",
        "\n",
        "    os.makedirs(dst_root, exist_ok=True)\n",
        "    moved, trimmed = 0, 0\n",
        "\n",
        "    for filename, n in files_fragments:\n",
        "        if min_acceptable <= n <= max_acceptable:\n",
        "            src_path = os.path.join(src_folder, filename)\n",
        "            dst_path = os.path.join(dst_root, filename)\n",
        "            try:\n",
        "                data = np.load(src_path)\n",
        "                while data.shape[0] - 2 >= target_length:\n",
        "                    data = data[1:-1]\n",
        "                if data.shape[0] > target_length:\n",
        "                    data = data[:-1]\n",
        "                np.save(dst_path, data)\n",
        "                moved += 1\n",
        "                trimmed += 1\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {filename}: {e}\")\n",
        "                continue\n",
        "    return moved, trimmed\n",
        "\n",
        "def compute_all_folder_means(root_path):\n",
        "    \"\"\"Returns a list of (folder_path, files_fragments, folder_mean).\"\"\"\n",
        "    folder_means = []\n",
        "    for folder_name in os.listdir(root_path):\n",
        "        folder_path = os.path.join(root_path, folder_name)\n",
        "        if os.path.isdir(folder_path):\n",
        "            files_fragments = get_fragments_per_file(folder_path)\n",
        "            folder_mean = calculate_folder_mean(files_fragments)\n",
        "            if folder_mean is not None:\n",
        "                folder_means.append((folder_path, files_fragments, folder_mean))\n",
        "    return folder_means\n",
        "\n",
        "def process_all_folders(root_path, destination_root):\n",
        "    # Step 1: compute folder-level means\n",
        "    folder_data = compute_all_folder_means(root_path)\n",
        "    if not folder_data:\n",
        "        print(\"No valid folders with .npy files found.\")\n",
        "        return\n",
        "\n",
        "    # Step 2: compute global mean and target length\n",
        "    individual_means = [mean for _, _, mean in folder_data]\n",
        "    global_mean = int(np.floor(np.mean(individual_means)))\n",
        "    target_length = global_mean - 2\n",
        "    print(f\"Global mean = {global_mean} | Target fragment count = {target_length}\")\n",
        "\n",
        "    # Step 3: process all folders\n",
        "    total_moved, total_trimmed = 0, 0\n",
        "    for folder_path, files_fragments, _ in folder_data:\n",
        "        print(f\"\\nProcessing: {os.path.basename(folder_path)}\")\n",
        "        moved, trimmed = move_and_trim_files_to_target(\n",
        "            folder_path, destination_root, files_fragments, global_mean, target_length\n",
        "        )\n",
        "        total_moved += moved\n",
        "        total_trimmed += trimmed\n",
        "        print(f\"Moved to train_good: {moved} files\")\n",
        "\n",
        "    print(f\"\\nAll Done!\")\n",
        "    print(f\"Total files saved in '{destination_root}': {total_moved}\")\n",
        "    print(f\"Total files trimmed to {target_length} fragments: {total_trimmed}\")\n",
        "\n",
        "\n",
        "# Set your source and destination root folders here\n",
        "source_root_path = dir + \"train/\"\n",
        "destination_root_path = dir + \"train_good/\"\n",
        "\n",
        "# Run the process\n",
        "process_all_folders(source_root_path, destination_root_path)"
      ],
      "metadata": {
        "id": "EOUxHudu0snF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8f09b2-f21c-4170-bb28-f7388b581f17"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Global mean = 12 | Target fragment count = 10\n",
            "\n",
            "Processing: redcro\n",
            "Moved to train_good: 96 files\n",
            "\n",
            "Processing: gbwwre1\n",
            "Moved to train_good: 80 files\n",
            "\n",
            "Processing: norcar\n",
            "Moved to train_good: 123 files\n",
            "\n",
            "Processing: comrav\n",
            "Moved to train_good: 86 files\n",
            "\n",
            "Processing: sonspa\n",
            "Moved to train_good: 120 files\n",
            "\n",
            "Processing: houspa\n",
            "Moved to train_good: 104 files\n",
            "\n",
            "All Done!\n",
            "Total files saved in '/content/drive/Shared drives/Deep Learning Group G/UPF_Deep_Learning_2025/Final Project/Birdclef2021/train_good/': 609\n",
            "Total files trimmed to 10 fragments: 609\n",
            "Duplicate filenames: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def check_npy_fragment_counts(folder_path, expected_fragments=10):\n",
        "    \"\"\"Verifies that all .npy files in the folder have exactly expected_fragments.\"\"\"\n",
        "    if not os.path.exists(folder_path):\n",
        "        print(f\"Path does not exist: {folder_path}\")\n",
        "        return\n",
        "\n",
        "    total_files = 0\n",
        "    valid_files = 0\n",
        "    invalid_files = []\n",
        "\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.npy'):\n",
        "            total_files += 1\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            try:\n",
        "                data = np.load(file_path)\n",
        "                if len(data.shape) == 3 and data.shape[0] == expected_fragments:\n",
        "                    valid_files += 1\n",
        "                else:\n",
        "                    invalid_files.append((filename, data.shape))\n",
        "            except Exception as e:\n",
        "                invalid_files.append((filename, f\"Error: {e}\"))\n",
        "\n",
        "    # Summary\n",
        "    print(f\"\\nChecked {total_files} .npy files in: {folder_path}\")\n",
        "    print(f\"Valid files with {expected_fragments} spectrograms: {valid_files}\")\n",
        "    print(f\"Invalid files: {len(invalid_files)}\")\n",
        "\n",
        "    if invalid_files:\n",
        "        print(\"\\nFiles with incorrect fragment counts or errors:\")\n",
        "        for name, issue in invalid_files:\n",
        "            print(f\" - {name}: {issue}\")\n",
        "\n",
        "# ----------------------------\n",
        "# Set your folder path here:\n",
        "folder = dir + \"train_good/\"\n",
        "check_npy_fragment_counts(folder , expected_fragments = 10)"
      ],
      "metadata": {
        "id": "6zRGOpaf4QbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0cf691f-cf97-499e-88e7-615fa6bd6429"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Checked 609 .npy files in: /content/drive/Shared drives/Deep Learning Group G/UPF_Deep_Learning_2025/Final Project/Birdclef2021/train_good/\n",
            "Valid files with 10 spectrograms: 609\n",
            "Invalid files: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Next step -> Prepare CSV"
      ],
      "metadata": {
        "id": "rWfX7JIwA4uF"
      }
    }
  ]
}